apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-ml-train-jobs
  namespace: spark
data:
  common.py: |
    import os

    from pyspark.sql import SparkSession


    def _env(name: str, default: str) -> str:
        v = os.getenv(name)
        return v if v not in (None, "") else default


    def build_spark(app_name: str) -> SparkSession:
        endpoint = _env("MINIO_ENDPOINT", "http://minio.minio.svc.cluster.local:9000")
        access_key = _env("MINIO_ACCESS_KEY", "minioadmin")
        secret_key = _env("MINIO_SECRET_KEY", "minioadmin")

        path_style = _env("S3A_PATH_STYLE_ACCESS", "true")
        packages = os.getenv("SPARK_S3_PACKAGES")

        builder = (
            SparkSession.builder.appName(app_name)
            .config("spark.sql.session.timeZone", "UTC")
            .config("spark.hadoop.fs.s3a.endpoint", endpoint)
            .config("spark.hadoop.fs.s3a.access.key", access_key)
            .config("spark.hadoop.fs.s3a.secret.key", secret_key)
            .config("spark.hadoop.fs.s3a.path.style.access", path_style)
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        )

        if packages:
            builder = builder.config("spark.jars.packages", packages)

        return builder.getOrCreate()

  ml_train_house_price.py: |
    import argparse
    import os
    from datetime import datetime, timezone

    from pyspark.ml import Pipeline
    from pyspark.ml.evaluation import RegressionEvaluator
    from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler
    from pyspark.ml.regression import RandomForestRegressor
    from pyspark.sql import functions as F

    from common import build_spark


    def _env(name: str, default: str) -> str:
        v = os.getenv(name)
        return v if v not in (None, "") else default


    def main() -> None:
        parser = argparse.ArgumentParser()
        parser.add_argument("--bucket", default=_env("MINIO_BUCKET", "house-lake"))
        parser.add_argument("--silver-prefix", default=_env("SILVER_PREFIX", "silver"))
        parser.add_argument("--model-prefix", default=_env("MODEL_PREFIX", "models/house_price"))
        parser.add_argument("--seed", type=int, default=int(_env("ML_SEED", "42")))
        parser.add_argument("--train-ratio", type=float, default=float(_env("ML_TRAIN_RATIO", "0.8")))

        parser.add_argument("--write-postgres", action="store_true")
        parser.add_argument(
            "--pg-url",
            default=_env(
                "POSTGRES_JDBC_URL",
                "jdbc:postgresql://postgres.postgres.svc.cluster.local:5432/house_warehouse",
            ),
        )
        parser.add_argument("--pg-user", default=_env("POSTGRES_USER", "postgres"))
        parser.add_argument("--pg-password", default=_env("POSTGRES_PASSWORD", "postgres"))

        args = parser.parse_args()

        spark = build_spark("ml_train_house_price")

        silver_path = f"s3a://{args.bucket}/{args.silver_prefix}/"
        df = spark.read.parquet(silver_path)

        df = (
          df.select(
            F.col("created_at"),
            F.col("id"),
            F.col("price").cast("double").alias("label"),
            F.col("sqft").cast("double").alias("sqft"),
            F.col("bedrooms").cast("double").alias("bedrooms"),
            F.col("bathrooms").cast("double").alias("bathrooms"),
            F.col("year_built").cast("double").alias("year_built"),
            F.col("location"),
            F.col("condition"),
          )
          .dropna(subset=["label", "location", "year_built"])
          .fillna({"sqft": 0.0, "bedrooms": 0.0, "bathrooms": 0.0, "condition": "Unknown"})
        )

        location_indexer = StringIndexer(inputCol="location", outputCol="location_idx", handleInvalid="keep")
        condition_indexer = StringIndexer(inputCol="condition", outputCol="condition_idx", handleInvalid="keep")
        encoder = OneHotEncoder(
            inputCols=["location_idx", "condition_idx"],
            outputCols=["location_ohe", "condition_ohe"],
            handleInvalid="keep",
        )
        assembler = VectorAssembler(
            inputCols=["sqft", "bedrooms", "bathrooms", "year_built", "location_ohe", "condition_ohe"],
            outputCol="features",
            handleInvalid="keep",
        )

        rf = RandomForestRegressor(
            featuresCol="features",
            labelCol="label",
            predictionCol="prediction",
            seed=args.seed,
            numTrees=int(_env("RF_NUM_TREES", "50")),
            maxDepth=int(_env("RF_MAX_DEPTH", "10")),
        )

        pipeline = Pipeline(stages=[location_indexer, condition_indexer, encoder, assembler, rf])

        train_df, test_df = df.randomSplit([args.train_ratio, 1.0 - args.train_ratio], seed=args.seed)
        model = pipeline.fit(train_df)

        pred = model.transform(test_df)
        rmse = float(RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="rmse").evaluate(pred))
        r2 = float(RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="r2").evaluate(pred))

        run_id = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")

        latest_path = f"s3a://{args.bucket}/{args.model_prefix.rstrip('/')}/latest"
        versioned_path = f"s3a://{args.bucket}/{args.model_prefix.rstrip('/')}/runs/{run_id}"

        model.write().overwrite().save(latest_path)
        model.write().overwrite().save(versioned_path)

        metrics = spark.createDataFrame(
            [
                {
                    "run_id": run_id,
                    "rmse": rmse,
                    "r2": r2,
                    "model_path": latest_path,
                    "versioned_model_path": versioned_path,
                    "as_of_utc": datetime.now(timezone.utc),
                }
            ]
        )

        metrics_path = f"s3a://{args.bucket}/{args.model_prefix.rstrip('/')}/metrics/"
        metrics.write.mode("append").parquet(metrics_path)

        if args.write_postgres:
            props = {"user": args.pg_user, "password": args.pg_password, "driver": "org.postgresql.Driver"}
            metrics.write.mode("append").jdbc(url=args.pg_url, table="ml_house_price_model_metrics", properties=props)

        print(f"[ml_train_house_price] run_id={run_id} rmse={rmse:.4f} r2={r2:.4f}")
        print(f"[ml_train_house_price] saved latest model to {latest_path}")

        spark.stop()


    if __name__ == "__main__":
        main()

---
apiVersion: batch/v1
kind: Job
metadata:
  name: house-price-train
  namespace: spark
spec:
  backoffLimit: 1
  template:
    spec:
      restartPolicy: Never
      volumes:
        - name: jobs
          configMap:
            name: spark-ml-train-jobs
        - name: ivy-cache
          emptyDir: {}
      containers:
        - name: spark-submit
          image: apache/spark:3.5.1
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
          volumeMounts:
            - name: jobs
              mountPath: /opt/jobs
            - name: ivy-cache
              mountPath: /tmp/ivy
          env:
            - name: DEBIAN_FRONTEND
              value: noninteractive
            - name: MINIO_ENDPOINT
              value: "http://minio.minio.svc.cluster.local:9000"
            - name: MINIO_ACCESS_KEY
              value: "minioadmin"
            - name: MINIO_SECRET_KEY
              value: "minioadmin"
            - name: MINIO_BUCKET
              value: "house-lake"
            - name: POSTGRES_JDBC_URL
              value: "jdbc:postgresql://postgres.postgres.svc.cluster.local:5432/house_warehouse"
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_PASSWORD
              value: "postgres"
          command:
            - /bin/sh
            - -lc
          args:
            - |
              set -e
              python3 -c "import numpy" 2>/dev/null || (apt-get update -y && apt-get install -y python3-numpy)
              /opt/spark/bin/spark-submit \
                --conf spark.jars.ivy=/tmp/ivy \
                --py-files /opt/jobs/common.py \
                --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.7.4 \
                /opt/jobs/ml_train_house_price.py \
                --bucket house-lake \
                --write-postgres
