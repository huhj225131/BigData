# Job chạy ngay khi apply (chỉ 1 lần)
apiVersion: batch/v1
kind: Job
metadata:
  name: ml-train-init
  namespace: spark
spec:
  backoffLimit: 1
  template:
    spec:
      restartPolicy: Never
      volumes:
        - name: jobs
          configMap:
            name: spark-ml-train-jobs
        - name: ivy-cache
          emptyDir: {}
      containers:
        - name: spark-submit
          image: apache/spark:3.5.1
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
          volumeMounts:
            - name: jobs
              mountPath: /opt/jobs
            - name: ivy-cache
              mountPath: /tmp/ivy
          env:
            - name: DEBIAN_FRONTEND
              value: noninteractive
            - name: MINIO_ENDPOINT
              value: "http://minio.minio.svc.cluster.local:9000"
            - name: MINIO_ACCESS_KEY
              value: "minioadmin"
            - name: MINIO_SECRET_KEY
              value: "minioadmin"
            - name: MINIO_BUCKET
              value: "house-lake"
            - name: POSTGRES_JDBC_URL
              value: "jdbc:postgresql://postgres.postgres.svc.cluster.local:5432/house_warehouse"
            - name: POSTGRES_USER
              value: "postgres"
            - name: POSTGRES_PASSWORD
              value: "postgres"
          command:
            - /bin/sh
            - -lc
          args:
            - |
              set -e
              echo "=== Initial ML Train + Inference ==="
              python3 -c "import numpy" 2>/dev/null || (apt-get update -y && apt-get install -y python3-numpy)
              
              echo "=== [1/2] Training ML Model ==="
              /opt/spark/bin/spark-submit \
                --conf spark.jars.ivy=/tmp/ivy \
                --py-files /opt/jobs/common.py \
                --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.7.4 \
                /opt/jobs/ml_train_house_price.py \
                --bucket house-lake \
                --write-postgres
              
              echo "=== [2/2] Running Inference on Silver ==="
              /opt/spark/bin/spark-submit \
                --conf spark.jars.ivy=/tmp/ivy \
                --py-files /opt/jobs/common.py \
                --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.7.4 \
                /opt/jobs/ml_inference_house_price.py \
                --bucket house-lake \
                --write-postgres
              
              echo "=== ✓ Initial ML Train + Inference completed ==="

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-ml-train-jobs
  namespace: spark
data:
  common.py: |
    import os

    from pyspark.sql import SparkSession


    def _env(name: str, default: str) -> str:
        v = os.getenv(name)
        return v if v not in (None, "") else default


    def build_spark(app_name: str) -> SparkSession:
        endpoint = _env("MINIO_ENDPOINT", "http://minio.minio.svc.cluster.local:9000")
        access_key = _env("MINIO_ACCESS_KEY", "minioadmin")
        secret_key = _env("MINIO_SECRET_KEY", "minioadmin")

        path_style = _env("S3A_PATH_STYLE_ACCESS", "true")
        packages = os.getenv("SPARK_S3_PACKAGES")

        builder = (
            SparkSession.builder.appName(app_name)
            .config("spark.sql.session.timeZone", "UTC")
            .config("spark.hadoop.fs.s3a.endpoint", endpoint)
            .config("spark.hadoop.fs.s3a.access.key", access_key)
            .config("spark.hadoop.fs.s3a.secret.key", secret_key)
            .config("spark.hadoop.fs.s3a.path.style.access", path_style)
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        )

        if packages:
            builder = builder.config("spark.jars.packages", packages)

        return builder.getOrCreate()

  ml_train_house_price.py: |
    import argparse
    import os
    from datetime import datetime, timezone

    from pyspark.ml import Pipeline
    from pyspark.ml.evaluation import RegressionEvaluator
    from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler
    from pyspark.ml.regression import RandomForestRegressor
    from pyspark.sql import functions as F

    from common import build_spark


    def _env(name: str, default: str) -> str:
        v = os.getenv(name)
        return v if v not in (None, "") else default


    def main() -> None:
        parser = argparse.ArgumentParser()
        parser.add_argument("--bucket", default=_env("MINIO_BUCKET", "house-lake"))
        parser.add_argument("--silver-prefix", default=_env("SILVER_PREFIX", "silver"))
        parser.add_argument("--model-prefix", default=_env("MODEL_PREFIX", "models/house_price"))
        parser.add_argument("--seed", type=int, default=int(_env("ML_SEED", "42")))
        parser.add_argument("--train-ratio", type=float, default=float(_env("ML_TRAIN_RATIO", "0.8")))

        parser.add_argument("--write-postgres", action="store_true")
        parser.add_argument(
            "--pg-url",
            default=_env(
                "POSTGRES_JDBC_URL",
                "jdbc:postgresql://postgres.postgres.svc.cluster.local:5432/house_warehouse",
            ),
        )
        parser.add_argument("--pg-user", default=_env("POSTGRES_USER", "postgres"))
        parser.add_argument("--pg-password", default=_env("POSTGRES_PASSWORD", "postgres"))

        args = parser.parse_args()

        spark = build_spark("ml_train_house_price")

        silver_path = f"s3a://{args.bucket}/{args.silver_prefix}/"
        df = spark.read.parquet(silver_path)

        # Load features from Silver (sử dụng features đã được tính sẵn + features gốc)
        df = (
            df.select(
                F.col("id"),
                F.col("created_at"),
                F.col("price").cast("double").alias("label"),
                # Original features
                F.col("sqft").cast("double").alias("sqft"),
                F.col("bedrooms").cast("double").alias("bedrooms"),
                F.col("bathrooms").cast("double").alias("bathrooms"),
                F.col("year_built").cast("double").alias("year_built"),
                F.col("location").alias("location"),
                F.col("condition").alias("condition"),
                # Pre-computed features from Silver
                F.col("price_per_sqft").cast("double").alias("price_per_sqft"),
                F.col("house_age").cast("double").alias("house_age"),
                F.col("total_rooms").cast("double").alias("total_rooms"),
                F.col("condition_score").cast("double").alias("condition_score"),
            )
            .dropna(subset=["label", "location", "year_built"])
            .fillna({
                "sqft": 0.0, "bedrooms": 0.0, "bathrooms": 0.0, "condition": "Unknown",
                "price_per_sqft": 0.0, "house_age": 0.0, "total_rooms": 0.0, "condition_score": 0.0
            })
        )

        # Categorical preprocessing
        location_indexer = StringIndexer(
            inputCol="location",
            outputCol="location_idx",
            handleInvalid="keep",
        )
        condition_indexer = StringIndexer(
            inputCol="condition",
            outputCol="condition_idx",
            handleInvalid="keep",
        )

        encoder = OneHotEncoder(
            inputCols=["location_idx", "condition_idx"],
            outputCols=["location_ohe", "condition_ohe"],
            handleInvalid="keep",
        )

        # Feature assembly: original features + 4 pre-computed features from Silver
        assembler = VectorAssembler(
            inputCols=[
                # Original numeric
                "sqft", "bedrooms", "bathrooms", "year_built",
                # Engineered features (4 cái)
                "price_per_sqft", "house_age", "total_rooms", "condition_score",
                # One-hot encoded categoricals
                "location_ohe", "condition_ohe"
            ],
            outputCol="features",
            handleInvalid="keep",
        )

        rf = RandomForestRegressor(
            featuresCol="features",
            labelCol="label",
            predictionCol="prediction",
            seed=args.seed,
            numTrees=int(_env("RF_NUM_TREES", "50")),
            maxDepth=int(_env("RF_MAX_DEPTH", "10")),
        )

        pipeline = Pipeline(stages=[
            location_indexer, condition_indexer, encoder, assembler, rf
        ])

        train_df, test_df = df.randomSplit([args.train_ratio, 1.0 - args.train_ratio], seed=args.seed)

        model = pipeline.fit(train_df)

        pred = model.transform(test_df)

        evaluator_rmse = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="rmse")
        evaluator_r2 = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="r2")

        rmse = float(evaluator_rmse.evaluate(pred))
        r2 = float(evaluator_r2.evaluate(pred))

        run_id = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")

        latest_path = f"s3a://{args.bucket}/{args.model_prefix.rstrip('/')}/latest"
        versioned_path = f"s3a://{args.bucket}/{args.model_prefix.rstrip('/')}/runs/{run_id}"

        model.write().overwrite().save(latest_path)
        model.write().overwrite().save(versioned_path)

        metrics = spark.createDataFrame(
            [
                {
                    "run_id": run_id,
                    "rmse": rmse,
                    "r2": r2,
                    "model_path": latest_path,
                    "versioned_model_path": versioned_path,
                    "as_of_utc": datetime.now(timezone.utc),
                }
            ]
        )

        metrics_path = f"s3a://{args.bucket}/{args.model_prefix.rstrip('/')}/metrics/"
        metrics.write.mode("append").parquet(metrics_path)

        if args.write_postgres:
            props = {"user": args.pg_user, "password": args.pg_password, "driver": "org.postgresql.Driver"}
            metrics.write.mode("append").jdbc(url=args.pg_url, table="ml_house_price_model_metrics", properties=props)

        print(f"[ml_train_house_price] run_id={run_id} rmse={rmse:.4f} r2={r2:.4f}")
        print(f"[ml_train_house_price] saved latest model to {latest_path}")

        spark.stop()


    if __name__ == "__main__":
        main()

  ml_inference_house_price.py: |
    import argparse
    import os
    from datetime import datetime, timezone

    from pyspark.ml import PipelineModel
    from pyspark.sql import functions as F

    from common import build_spark


    def _env(name: str, default: str) -> str:
        v = os.getenv(name)
        return v if v not in (None, "") else default


    def main() -> None:
        parser = argparse.ArgumentParser()
        parser.add_argument("--bucket", default=_env("MINIO_BUCKET", "house-lake"))
        parser.add_argument("--silver-prefix", default=_env("SILVER_PREFIX", "silver"))
        parser.add_argument("--gold-prefix", default=_env("GOLD_PREFIX", "gold"))

        parser.add_argument(
            "--model-path",
            default=_env("MODEL_PATH", ""),
            help="Full s3a:// path to a saved Spark PipelineModel. Defaults to s3a://<bucket>/models/house_price/latest",
        )

        parser.add_argument("--write-postgres", action="store_true")
        parser.add_argument(
            "--pg-url",
            default=_env(
                "POSTGRES_JDBC_URL",
                "jdbc:postgresql://postgres.postgres.svc.cluster.local:5432/house_warehouse",
            ),
        )
        parser.add_argument("--pg-user", default=_env("POSTGRES_USER", "postgres"))
        parser.add_argument("--pg-password", default=_env("POSTGRES_PASSWORD", "postgres"))

        args = parser.parse_args()

        spark = build_spark("ml_inference_house_price")

        silver_path = f"s3a://{args.bucket}/{args.silver_prefix}/"
        df = spark.read.parquet(silver_path)

        # Load features from Silver (giống training - đảm bảo consistency)
        df = (
            df.select(
                F.col("id"),
                F.col("created_at"),
                F.col("price").cast("double").alias("actual_price"),
                # Original features
                F.col("sqft").cast("double").alias("sqft"),
                F.col("bedrooms").cast("double").alias("bedrooms"),
                F.col("bathrooms").cast("double").alias("bathrooms"),
                F.col("year_built").cast("double").alias("year_built"),
                F.col("location").alias("location"),
                F.col("condition").alias("condition"),
                # Pre-computed features from Silver
                F.col("price_per_sqft").cast("double").alias("price_per_sqft"),
                F.col("house_age").cast("double").alias("house_age"),
                F.col("total_rooms").cast("double").alias("total_rooms"),
                F.col("condition_score").cast("double").alias("condition_score"),
            )
            .dropna(subset=["location", "year_built"])
            .fillna({
                "sqft": 0.0, "bedrooms": 0.0, "bathrooms": 0.0, "condition": "Unknown",
                "price_per_sqft": 0.0, "house_age": 0.0, "total_rooms": 0.0, "condition_score": 0.0
            })
        )

        # Load trained model (từ Training job)
        model_path = args.model_path.strip() or f"s3a://{args.bucket}/models/house_price/latest"
        model = PipelineModel.load(model_path)

        # Apply model to Silver data
        scored = model.transform(df)

        run_id = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")

        # Prepare output with predictions
        out = (
            scored.select(
                "id",
                "created_at",
                "actual_price",
                F.col("prediction").cast("double").alias("predicted_price"),
            )
            .withColumn("model_path", F.lit(model_path))
            .withColumn("run_id", F.lit(run_id))
            .withColumn("as_of_utc", F.current_timestamp())
        )

        pred_path = f"s3a://{args.bucket}/{args.gold_prefix}/predictions_house_price/"
        out.write.mode("append").parquet(pred_path)

        if args.write_postgres:
            props = {"user": args.pg_user, "password": args.pg_password, "driver": "org.postgresql.Driver"}
            out.write.mode("append").jdbc(
                url=args.pg_url,
                table="house_price_predictions",
                properties=props,
            )

        print(f"[ml_inference_house_price] run_id={run_id} model_path={model_path}")
        print(f"[ml_inference_house_price] wrote predictions to {pred_path}")

        spark.stop()


    if __name__ == "__main__":
        main()

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: house-price-train
  namespace: spark
spec:
  # Chạy mỗi giờ vào phút 0 (00:00, 01:00, 02:00, ...)
  schedule: "0 * * * *"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      backoffLimit: 1
      template:
        spec:
          restartPolicy: Never
          volumes:
            - name: jobs
              configMap:
                name: spark-ml-train-jobs
            - name: ivy-cache
              emptyDir: {}
          containers:
            - name: spark-submit
              image: apache/spark:3.5.1
              imagePullPolicy: IfNotPresent
              securityContext:
                runAsUser: 0
              volumeMounts:
                - name: jobs
                  mountPath: /opt/jobs
                - name: ivy-cache
                  mountPath: /tmp/ivy
              env:
                - name: DEBIAN_FRONTEND
                  value: noninteractive
                - name: MINIO_ENDPOINT
                  value: "http://minio.minio.svc.cluster.local:9000"
                - name: MINIO_ACCESS_KEY
                  value: "minioadmin"
                - name: MINIO_SECRET_KEY
                  value: "minioadmin"
                - name: MINIO_BUCKET
                  value: "house-lake"
                - name: POSTGRES_JDBC_URL
                  value: "jdbc:postgresql://postgres.postgres.svc.cluster.local:5432/house_warehouse"
                - name: POSTGRES_USER
                  value: "postgres"
                - name: POSTGRES_PASSWORD
                  value: "postgres"
              command:
                - /bin/sh
                - -lc
              args:
                - |
                  set -e
                  python3 -c "import numpy" 2>/dev/null || (apt-get update -y && apt-get install -y python3-numpy)
                  
                  echo "=== [1/2] Training ML Model ==="
                  /opt/spark/bin/spark-submit \
                    --conf spark.jars.ivy=/tmp/ivy \
                    --py-files /opt/jobs/common.py \
                    --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.7.4 \
                    /opt/jobs/ml_train_house_price.py \
                    --bucket house-lake \
                    --write-postgres
                  
                  echo "=== [2/2] Running Inference on Silver ==="
                  /opt/spark/bin/spark-submit \
                    --conf spark.jars.ivy=/tmp/ivy \
                    --py-files /opt/jobs/common.py \
                    --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.7.4 \
                    /opt/jobs/ml_inference_house_price.py \
                    --bucket house-lake \
                    --write-postgres
                  
                  echo "=== ✓ Train + Inference completed ==="
