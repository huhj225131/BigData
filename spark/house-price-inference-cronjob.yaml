apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-ml-jobs
  namespace: spark
data:
  common.py: |
    import os

    from pyspark.sql import SparkSession


    def _env(name: str, default: str) -> str:
        v = os.getenv(name)
        return v if v not in (None, "") else default


    def build_spark(app_name: str) -> SparkSession:
        endpoint = _env("MINIO_ENDPOINT", "http://minio.minio.svc.cluster.local:9000")
        access_key = _env("MINIO_ACCESS_KEY", "minioadmin")
        secret_key = _env("MINIO_SECRET_KEY", "minioadmin")

        path_style = _env("S3A_PATH_STYLE_ACCESS", "true")
        packages = os.getenv("SPARK_S3_PACKAGES")

        builder = (
            SparkSession.builder.appName(app_name)
            .config("spark.sql.session.timeZone", "UTC")
            .config("spark.hadoop.fs.s3a.endpoint", endpoint)
            .config("spark.hadoop.fs.s3a.access.key", access_key)
            .config("spark.hadoop.fs.s3a.secret.key", secret_key)
            .config("spark.hadoop.fs.s3a.path.style.access", path_style)
            .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
            .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        )

        if packages:
            builder = builder.config("spark.jars.packages", packages)

        return builder.getOrCreate()

  ml_inference_house_price.py: |
    import argparse
    import os
    from datetime import datetime, timezone

    from pyspark.ml import PipelineModel
    from pyspark.sql import functions as F

    from common import build_spark


    def _env(name: str, default: str) -> str:
        v = os.getenv(name)
        return v if v not in (None, "") else default


    def main() -> None:
        parser = argparse.ArgumentParser()
        parser.add_argument("--bucket", default=_env("MINIO_BUCKET", "house-lake"))
        parser.add_argument("--silver-prefix", default=_env("SILVER_PREFIX", "silver"))
        parser.add_argument("--gold-prefix", default=_env("GOLD_PREFIX", "gold"))

        parser.add_argument(
            "--model-path",
            default=_env("MODEL_PATH", ""),
            help="Full s3a:// path to a saved Spark PipelineModel. Defaults to s3a://<bucket>/models/house_price/latest",
        )

        parser.add_argument(
            "--dt",
            default=_env("INFER_DT", ""),
            help="Optional dt filter (YYYY-MM-DD). If not set, scores all available silver partitions.",
        )

        parser.add_argument("--write-postgres", action="store_true")
        parser.add_argument(
            "--pg-url",
            default=_env(
                "POSTGRES_JDBC_URL",
                "jdbc:postgresql://postgres.postgres.svc.cluster.local:5432/house_warehouse",
            ),
        )
        parser.add_argument("--pg-user", default=_env("POSTGRES_USER", "postgres"))
        parser.add_argument("--pg-password", default=_env("POSTGRES_PASSWORD", "postgres"))

        args = parser.parse_args()

        spark = build_spark("ml_inference_house_price")

        silver_path = f"s3a://{args.bucket}/{args.silver_prefix}/"
        df = spark.read.parquet(silver_path)

        if args.dt:
            df = df.filter(F.col("dt") == F.to_date(F.lit(args.dt)))

        df = df.select(
            F.col("ingest_time_utc"),
            F.col("topic"),
            F.col("partition"),
            F.col("offset"),
            F.col("id"),
            F.col("price").cast("double").alias("actual_price"),
            F.col("sqft").cast("double").alias("sqft"),
            F.col("bedrooms").cast("double").alias("bedrooms"),
            F.col("bathrooms").cast("double").alias("bathrooms"),
            F.col("year_built").cast("double").alias("year_built"),
            F.col("location"),
            F.col("condition"),
            F.col("dt"),
        ).dropna(subset=["location", "year_built"]).fillna(
            {"sqft": 0.0, "bedrooms": 0.0, "bathrooms": 0.0, "condition": "Unknown"}
        )

        model_path = args.model_path.strip() or f"s3a://{args.bucket}/models/house_price/latest"
        model = PipelineModel.load(model_path)

        scored = model.transform(df)

        run_id = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")

        out = (
            scored.select(
                "ingest_time_utc",
                "topic",
                "partition",
                "offset",
                "id",
                "dt",
                "actual_price",
                F.col("prediction").cast("double").alias("predicted_price"),
            )
            .withColumn("model_path", F.lit(model_path))
            .withColumn("run_id", F.lit(run_id))
            .withColumn("as_of_utc", F.current_timestamp())
        )

        pred_path = f"s3a://{args.bucket}/{args.gold_prefix}/predictions_house_price/"
        out.write.mode("append").partitionBy("dt").parquet(pred_path)

        if args.write_postgres:
            props = {"user": args.pg_user, "password": args.pg_password, "driver": "org.postgresql.Driver"}
            out.write.mode("append").jdbc(
                url=args.pg_url,
                table="house_price_predictions",
                properties=props,
            )

        print(f"[ml_inference_house_price] run_id={run_id} model_path={model_path}")
        print(f"[ml_inference_house_price] wrote predictions to {pred_path}")

        spark.stop()


    if __name__ == "__main__":
        main()

---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: house-price-inference
  namespace: spark
spec:
  # Default: every 5 minutes. Change to */X * * * * as needed.
  schedule: "*/5 * * * *"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      backoffLimit: 1
      template:
        spec:
          restartPolicy: Never
          volumes:
            - name: jobs
              configMap:
                name: spark-ml-jobs
            - name: ivy-cache
              emptyDir: {}
          containers:
            - name: spark-submit
              image: apache/spark:3.5.1
              imagePullPolicy: IfNotPresent
              securityContext:
                runAsUser: 0
              volumeMounts:
                - name: jobs
                  mountPath: /opt/jobs
                - name: ivy-cache
                  mountPath: /tmp/ivy
              env:
                - name: DEBIAN_FRONTEND
                  value: noninteractive
                - name: MINIO_ENDPOINT
                  value: "http://minio.minio.svc.cluster.local:9000"
                - name: MINIO_ACCESS_KEY
                  value: "minioadmin"
                - name: MINIO_SECRET_KEY
                  value: "minioadmin"
                - name: MINIO_BUCKET
                  value: "house-lake"
                - name: POSTGRES_JDBC_URL
                  value: "jdbc:postgresql://postgres.postgres.svc.cluster.local:5432/house_warehouse"
                - name: POSTGRES_USER
                  value: "postgres"
                - name: POSTGRES_PASSWORD
                  value: "postgres"
              command:
                - /bin/sh
                - -lc
              args:
                - |
                  set -e
                  python3 -c "import numpy" 2>/dev/null || (apt-get update -y && apt-get install -y python3-numpy)
                  /opt/spark/bin/spark-submit \
                    --conf spark.jars.ivy=/tmp/ivy \
                    --py-files /opt/jobs/common.py \
                    --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.postgresql:postgresql:42.7.4 \
                    /opt/jobs/ml_inference_house_price.py \
                    --bucket house-lake \
                    --write-postgres
