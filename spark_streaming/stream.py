from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, when, expr
from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType
from pyspark.sql.functions import lit
import os

# --- C·∫§U H√åNH ---
KAFKA_BROKER = os.getenv("KAFKA_BROKER", "kafka-service.kafka.svc.cluster.local:9092")
TOPIC_NAME = os.getenv("KAFKA_TOPIC", "data-stream")

# PostgreSQL Config
POSTGRES_HOST = os.getenv("POSTGRES_HOST", "postgres.postgres.svc.cluster.local")
POSTGRES_PORT = os.getenv("POSTGRES_PORT", "5432")
POSTGRES_DB = os.getenv("POSTGRES_DB", "house_warehouse")
POSTGRES_USER = os.getenv("POSTGRES_USER", "postgres")
POSTGRES_PASSWORD = os.getenv("POSTGRES_PASSWORD", "postgres")

# Checkpoint (d√πng MinIO cho production)
CHECKPOINT_DIR = os.getenv("CHECKPOINT_DIR", "/tmp/spark-checkpoint")

spark = SparkSession.builder \
    .appName("HouseDataStreamingCleaner") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# Schema kh·ªõp v·ªõi d·ªØ li·ªáu house_data c·ªßa b·∫°n
schema = StructType() \
    .add("id", IntegerType()) \
    .add("price", DoubleType()) \
    .add("sqft", IntegerType()) \
    .add("bedrooms", IntegerType()) \
    .add("bathrooms", DoubleType()) \
    .add("location", StringType()) \
    .add("year_built", IntegerType()) \
    .add("condition", StringType())

# 1. ƒê·ªçc lu·ªìng t·ª´ Kafka
raw_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BROKER) \
    .option("subscribe", TOPIC_NAME) \
    .load()

# 2. Parse JSON
df = raw_df.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")

# 3. --- LOGIC L√ÄM S·∫†CH (CHUY·ªÇN T·ª™ PANDAS) ---

# B∆∞·ªõc A: X·ª≠ l√Ω missing values (fillna)
# V√¨ l√† streaming, ta th∆∞·ªùng fill b·∫±ng c√°c h·∫±ng s·ªë ho·∫∑c gi√° tr·ªã trung b√¨nh ƒë√£ bi·∫øt tr∆∞·ªõc
cleaned_df = df.na.fill({
    "price": 0.0,
    "sqft": 0,
    "location": "Unknown",
    "condition": "Good"
})

# B∆∞·ªõc B: X·ª≠ l√Ω Outlier (IQR thay th·∫ø b·∫±ng Filter)
# Trong streaming, IQR r·∫•t kh√≥ t√≠nh real-time. 
# Thay v√†o ƒë√≥ ta d√πng ng∆∞·ª°ng nghi·ªáp v·ª• (Domain Knowledge) 
# V√≠ d·ª•: Gi√° nh√† th∆∞·ªùng t·ª´ 50k ƒë·∫øn 5M, di·ªán t√≠ch > 100 sqft
lower_bound = 50000 
upper_bound = 2000000 

cleaned_df = cleaned_df.filter(
    (col("price") >= lower_bound) & 
    (col("price") <= upper_bound) &
    (col("sqft") > 0)
)



# PostgreSQL Connection
db_properties = {
    "user": POSTGRES_USER,
    "password": POSTGRES_PASSWORD,
    "driver": "org.postgresql.Driver"
}
jdbc_url = f"jdbc:postgresql://{POSTGRES_HOST}:{POSTGRES_PORT}/{POSTGRES_DB}"

TARGET_TABLE = os.getenv("POSTGRES_TABLE", "house_data_speed") 

def write_to_postgres(df, epoch_id):
    """Ghi batch v√†o PostgreSQL v·ªõi metadata"""
    try:
        # Th√™m c·ªôt ƒë·ªÉ sau n√†y bi·∫øt d√≤ng n√†y t·ª´ lu·ªìng Stream ƒë·∫©y v√†o
        df_with_source = df.withColumn("created_at", expr("current_timestamp()"))
        count = df_with_source.count()
        if count > 0:
            df_with_source.write.jdbc(
                url=jdbc_url, 
                table=TARGET_TABLE,
                mode="append", 
                properties=db_properties
            )
            print(f"[Batch {epoch_id}] ƒê√£ ghi {count} records v√†o PostgreSQL")
        else:
            print(f"[Batch {epoch_id}] Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ ghi")
    except Exception as e:
        print(f"[Batch {epoch_id}] L·ªói ghi PostgreSQL: {e}")
        raise

# K√≠ch ho·∫°t ghi d·ªØ li·ªáu
print(f"üöÄ Kh·ªüi ƒë·ªông Spark Streaming...")
print(f"   Kafka: {KAFKA_BROKER}")
print(f"   Topic: {TOPIC_NAME}")
print(f"   PostgreSQL: {jdbc_url}")
print(f"   Checkpoint: {CHECKPOINT_DIR}")

query = cleaned_df.writeStream \
    .foreachBatch(write_to_postgres) \
    .option("checkpointLocation", CHECKPOINT_DIR) \
    .trigger(processingTime='5 seconds') \
    .start()

print("‚úì Spark Streaming ƒëang ch·∫°y... Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng.")
query.awaitTermination()