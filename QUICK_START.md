# ğŸš€ QUICK START - BigData Pipeline

**Lambda Architecture:** Batch Layer (CronJob) + Speed Layer (Streaming) cháº¡y tá»± Ä‘á»™ng

---

## ğŸ“‹ Prerequisites

- âœ… Minikube running (`minikube start`)
- âœ… kubectl configured
- âœ… Äang á»Ÿ thÆ° má»¥c root: `D:\2025.1\bigdata\btl_bigdata`
- âœ… MinIO bucket `house-lake` Ä‘Ã£ táº¡o

---

## ğŸ—ï¸ BÆ¯á»šC 1: Deploy Infrastructure (1 láº§n)

```powershell
# 1.1. MinIO
kubectl create namespace minio
kubectl apply -f minio/config_minio.yaml
kubectl wait --for=condition=Ready pod -l app=minio -n minio --timeout=90s

# 1.2. PostgreSQL
kubectl apply -f postgres/postgres.yaml
kubectl wait --for=condition=Ready pod -l app=postgres -n postgres --timeout=90s

# 1.3. Kafka
kubectl create namespace kafka
kubectl apply -f kafka/kafka.yaml
kubectl wait --for=condition=Ready pod/kafka-0 -n kafka --timeout=120s

# 1.4. Spark namespace (cho CronJobs)
kubectl create namespace spark

# 1.5. Kafka Flow (Producer + Consumer + Spark Streaming)
kubectl apply -f kafka/flow.yaml
kubectl apply -f spark_streaming/spark-deployment.yaml
Start-Sleep -Seconds 30

# Verify
kubectl get pods -A | Select-String -Pattern "(kafka|minio|postgres|spark)"
```

**Expected: Táº¥t cáº£ pods `Running`**

---

## ğŸ”§ BÆ¯á»šC 2: Copy Code vÃ o Kafka Pods

```powershell
# 2.1. Producer
$POD_PRO = kubectl get pods -n kafka -l app=producer -o jsonpath='{.items[0].metadata.name}'
kubectl cp kafka/producer.py -n kafka "${POD_PRO}:/app/producer.py"
kubectl cp kafka/house_data.json -n kafka "${POD_PRO}:/app/house_data.json"

# 2.2. Consumer
$POD_CON = kubectl get pods -n kafka -l app=consumer -o jsonpath='{.items[0].metadata.name}'
kubectl cp kafka/consumer.py -n kafka "${POD_CON}:/app/consumer.py"
kubectl cp kafka/upload_to_storage.py -n kafka "${POD_CON}:/app/upload_to_storage.py"

# 2.3. Spark Streaming
$POD_STREAM = kubectl get pods -n kafka -l app=spark-streaming -o jsonpath='{.items[0].metadata.name}'
kubectl cp spark_streaming/stream.py -n kafka "${POD_STREAM}:/app/stream.py"

Write-Host "âœ… Code copied!" -ForegroundColor Green
```

---

## ğŸ¯ BÆ¯á»šC 3: Táº¡o MinIO Bucket

```powershell
# Terminal riÃªng - Port-forward MinIO
kubectl -n minio port-forward svc/minio-public 9001:9001
```

**Má»Ÿ browser:**
- URL: `http://localhost:9001`
- Login: `minioadmin` / `minioadmin`
- Táº¡o bucket tÃªn: **`house-lake`**

---

## ğŸš€ BÆ¯á»šC 4: Deploy Spark Pipeline (Cháº¡y ngay + Auto schedule)

```powershell
# Deploy Batch Pipeline + ML Pipeline (sáº½ cháº¡y ngay láº­p tá»©c)
kubectl apply -f spark/batch-pipeline-cronjob.yaml
kubectl apply -f spark/house-price-train-job.yaml

# Xem jobs Ä‘ang cháº¡y
kubectl get jobs -n spark
# OUTPUT:
# batch-pipeline-init    0/1    5s
# ml-train-init          0/1    3s

# Xem logs real-time
Write-Host "Watching Batch Pipeline Init..." -ForegroundColor Cyan
kubectl logs -n spark -l job-name=batch-pipeline-init --tail=100 -f

# Sau khi Batch xong, xem ML logs
Write-Host "Watching ML Train Init..." -ForegroundColor Cyan
kubectl logs -n spark -l job-name=ml-train-init --tail=100 -f

# Verify CronJobs Ä‘Ã£ Ä‘Æ°á»£c táº¡o
kubectl get cronjob -n spark
# OUTPUT:
# NAME                SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE
# batch-pipeline      */10 * * * *  False     0        2m
# house-price-train   0 * * * *     False     0        5m
```

**Giáº£i thÃ­ch:**
- âœ… `batch-pipeline-init` cháº¡y ngay (Bronze â†’ Silver â†’ Gold)
- âœ… `ml-train-init` cháº¡y ngay (Train â†’ Inference)
- â° `batch-pipeline` CronJob tá»± Ä‘á»™ng cháº¡y má»—i 10 phÃºt
- â° `house-price-train` CronJob tá»± Ä‘á»™ng cháº¡y má»—i giá»

---

## ğŸ“¡ BÆ¯á»šC 5: Cháº¡y Kafka Producer & Consumer (Manual)

### Má» 3 TERMINAL POWERSHELL Má»šI:

---

### **TERMINAL 1: PRODUCER (Data Source)**

```powershell
cd D:\2025.1\bigdata\btl_bigdata
$POD_PRO = kubectl get pods -n kafka -l app=producer -o jsonpath='{.items[0].metadata.name}'
kubectl exec -it -n kafka $POD_PRO -- python /app/producer.py
```

**Output mong Ä‘á»£i:**
```
âœ“ Gá»­i thÃ nh cÃ´ng tá»›i data-stream [0] @ offset 0
âœ“ Gá»­i thÃ nh cÃ´ng tá»›i data-stream [0] @ offset 1
...
```

**Äá»‚ CHáº Y 1-2 PHÃšT** (gá»­i ~300-500 messages)

---

### **TERMINAL 2: CONSUMER (Batch Pipeline â†’ Bronze)**

```powershell
cd D:\2025.1\bigdata\btl_bigdata
$POD_CON = kubectl get pods -n kafka -l app=consumer -o jsonpath='{.items[0].metadata.name}'
kubectl exec -it -n kafka $POD_CON -- python /app/consumer.py
```

**Output mong Ä‘á»£i:**
```
âœ… [BRONZE] Uploaded 200 records -> s3://house-lake/bronze/dt=2026-01-16/...
âœ… [BRONZE] Uploaded 200 records -> s3://house-lake/bronze/dt=2026-01-16/...
```

**Äá»‚ CHáº Y cho Ä‘áº¿n khi tháº¥y Ã­t nháº¥t 2-3 batch uploaded**

---

### **TERMINAL 3: SPARK STREAMING (Speed Pipeline â†’ PostgreSQL)**

```powershell
cd D:\2025.1\bigdata\btl_bigdata
$POD_STREAM = kubectl get pods -n kafka -l app=spark-streaming -o jsonpath='{.items[0].metadata.name}'

kubectl exec -it -n kafka $POD_STREAM -- /bin/bash -c "mkdir -p /tmp/ivy2 && /opt/spark/bin/spark-submit --master local[2] --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.1 --conf spark.jars.ivy=/tmp/ivy2 /app/stream.py"
```

**Output mong Ä‘á»£i:**
```
ğŸš€ Khá»Ÿi Ä‘á»™ng Spark Streaming...
   Kafka: kafka-service.kafka.svc.cluster.local:9092
   PostgreSQL: jdbc:postgresql://postgres.postgres...
[Batch 0] ÄÃ£ ghi 15 records vÃ o PostgreSQL
[Batch 1] ÄÃ£ ghi 23 records vÃ o PostgreSQL
...
```

**Äá»‚ CHáº Y liÃªn tá»¥c** - Ä‘Ã¢y lÃ  real-time pipeline!

---

## ğŸ“Š BÆ¯á»šC 6: Verify Data

### 6.1. Check MinIO (Browser)

URL: `http://localhost:9001` (minioadmin/minioadmin)

**Expected folders in `house-lake`:**
- âœ… `bronze/dt=2026-01-16/...` - Raw data tá»« Kafka
- âœ… `silver/` - Cleaned data
- âœ… `gold/location_stats/`, `gold/year_trend/` - Aggregations
- âœ… `models/house_price/latest/` - ML model (náº¿u Ä‘Ã£ train)

---

### 6.2. Check PostgreSQL

```powershell
# Terminal má»›i - Port-forward
kubectl -n postgres port-forward svc/postgres 5433:5432
```

**Connect DBeaver:**
- Host: `localhost`
- Port: **5433**
- Database: `house_warehouse`
- User/Pass: `postgres` / `postgres`

**Run SQL:**

```sql
-- Speed Layer (Real-time tá»« Streaming)
SELECT COUNT(*) FROM house_data_speed;
SELECT * FROM house_data_speed ORDER BY created_at DESC LIMIT 10;

-- Batch Layer (tá»« Silver job)
SELECT COUNT(*) FROM fact_house;
SELECT location, COUNT(*) FROM fact_house GROUP BY location;

-- Gold Layer (Aggregations)
SELECT * FROM gold_location_stats ORDER BY avg_price DESC LIMIT 5;
SELECT * FROM gold_year_trend ORDER BY year_built DESC LIMIT 5;

-- ML Predictions (náº¿u Ä‘Ã£ cháº¡y)
SELECT COUNT(*), run_id FROM house_price_predictions GROUP BY run_id;
```

---

## ğŸ›‘ BÆ¯á»šC 7: Stop & Cleanup

```powershell
# Stop Producer/Consumer/Streaming (Ctrl+C in cÃ¡c Terminal)

# Pause CronJobs (khÃ´ng xÃ³a, chá»‰ táº¡m dá»«ng)
kubectl patch cronjob batch-pipeline -n spark -p '{"spec":{"suspend":true}}'
kubectl patch cronjob house-price-train -n spark -p '{"spec":{"suspend":true}}'

# Hoáº·c xÃ³a hoÃ n toÃ n
kubectl delete cronjob batch-pipeline house-price-train -n spark
kubectl delete job batch-pipeline-init ml-train-init -n spark
kubectl delete configmap batch-pipeline-config spark-ml-train-jobs -n spark

# Scale down Kafka pods (optional)
kubectl scale -n kafka deploy/producer-data --replicas=0
kubectl scale -n kafka deploy/consumer-logger --replicas=0
kubectl scale -n kafka deploy/spark-streaming --replicas=0
```

---

## ğŸ“ˆ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Producer â†’ Kafka (data-stream)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                  â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPEED    â”‚   â”‚  BATCH LAYER (Auto)        â”‚
â”‚  LAYER    â”‚   â”‚                            â”‚
â”‚ (Manual)  â”‚   â”‚  Consumer â†’ Bronze         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚      â†“                     â”‚
â”‚ Spark     â”‚   â”‚  [Init Job - cháº¡y ngay]    â”‚
â”‚ Streaming â”‚   â”‚  batch-pipeline-init       â”‚
â”‚    â†“      â”‚   â”‚  - Silver (Clean+Features) â”‚
â”‚ PostgreSQLâ”‚   â”‚  - Gold (Aggregations)     â”‚
â”‚ house_    â”‚   â”‚      â†“                     â”‚
â”‚ data_speedâ”‚   â”‚  [CronJob - má»—i 10 phÃºt]   â”‚
â”‚           â”‚   â”‚  batch-pipeline            â”‚
â”‚ <10s      â”‚   â”‚      â†“                     â”‚
â”‚           â”‚   â”‚  [Init Job - cháº¡y ngay]    â”‚
â”‚           â”‚   â”‚  ml-train-init             â”‚
â”‚           â”‚   â”‚  - Train + Inference       â”‚
â”‚           â”‚   â”‚      â†“                     â”‚
â”‚           â”‚   â”‚  [CronJob - má»—i giá»]       â”‚
â”‚           â”‚   â”‚  house-price-train         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   PostgreSQL         â”‚
    â”‚  - house_data_speed  â”‚
    â”‚  - fact_house        â”‚
    â”‚  - gold_* (4 tables) â”‚
    â”‚  - ml_metrics        â”‚
    â”‚  - predictions       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› Troubleshooting

### Producer khÃ´ng gá»­i Ä‘Æ°á»£c message
```powershell
kubectl logs -n kafka -l app=producer --tail=50
# Check Kafka connection
```

### Consumer khÃ´ng ghi Ä‘Æ°á»£c MinIO
```powershell
kubectl logs -n kafka -l app=consumer --tail=50
# Check MinIO bucket tá»“n táº¡i
```

### Silver job lá»—i "Path not found"
```powershell
# Äáº£m báº£o Consumer Ä‘Ã£ cháº¡y vÃ  upload Bronze
# Check MinIO console xem cÃ³ bronze/ folder
```

### Spark Streaming khÃ´ng ghi PostgreSQL
```powershell
kubectl logs -n kafka -l app=spark-streaming --tail=100
# Check PostgreSQL connection
```

---

## ğŸ¯ Success Criteria

âœ… **Init Jobs (Cháº¡y ngay khi deploy):**
- `batch-pipeline-init` hoÃ n thÃ nh: Bronze â†’ Silver â†’ Gold
- `ml-train-init` hoÃ n thÃ nh: Train model â†’ Inference
- PostgreSQL cÃ³ data trong `fact_house`, `gold_*`, `house_price_predictions`

âœ… **CronJobs (Auto schedule):**
- `batch-pipeline` CronJob táº¡o thÃ nh cÃ´ng (schedule: `*/10 * * * *`)
- `house-price-train` CronJob táº¡o thÃ nh cÃ´ng (schedule: `0 * * * *`)
- Jobs tá»± Ä‘á»™ng cháº¡y theo schedule

âœ… **Speed Pipeline (Manual):**
- Spark Streaming ghi data vÃ o `house_data_speed`
- Real-time latency < 10s

---

## ğŸ“ Notes

- **Init Jobs:** Cháº¡y 1 láº§n khi deploy, khÃ´ng retry tá»± Ä‘á»™ng náº¿u fail
- **CronJobs:** Tá»± Ä‘á»™ng cháº¡y theo schedule, cÃ³ retry náº¿u fail
- **Dedup strategy:** `pg-max-offset` - chá»‰ process offset má»›i tá»« Kafka
- **Manual trigger:** `kubectl create job --from=cronjob/batch-pipeline manual-$(date +%s) -n spark`
- **Performance:** Batch ~1-2 phÃºt, ML ~5-10 phÃºt

**Happy Data Engineering! ğŸš€**
